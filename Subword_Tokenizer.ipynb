{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hJR1IHbBIZwi",
        "0jxQL3Yge1x8",
        "Pw4wH-80Tc2Q",
        "V4l18UMSIrM3",
        "EVlGvrvrIw-A",
        "PwLDRaCnI5Tk",
        "zMyoaLOSI5Tn",
        "uKzhMjUaI5Tr",
        "F_dJ0W2GJLiD"
      ],
      "authorship_tag": "ABX9TyNifh0TNJdLj31vcUfK6Qwy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kim-Jeong-Ju/AI_Modeling_NLP/blob/main/Subword_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Packages and Modules Importation**"
      ],
      "metadata": {
        "id": "hJR1IHbBIZwi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O_aUhXcuDV7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e8e074-4cdb-47b4-f782-d4cc2c56608f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 5.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.13.2\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "import csv\n",
        "import collections\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, Markdown, Latex\n",
        "\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "\n",
        "!pip install tokenizers\n",
        "from tokenizers import BertWordPieceTokenizer, ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Subword Tokenizer, 서브워드 토크나이저**"
      ],
      "metadata": {
        "id": "yzqZF2BTIyT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BPE(Byte Pair Encoding) in NLP, 자연어 처리에서의 BPE**"
      ],
      "metadata": {
        "id": "0jxQL3Yge1x8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_merges = 10         # BPE를 수행할 횟수\n",
        "dictionary = {\"l o w </w>\" : 5, \"l o w e r </w>\" : 2, \"n e w e s t </w>\" : 6, \"w i d e s t </w>\" : 3}\n",
        "\n",
        "def get_stats(dictionary):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in dictionary.items():\n",
        "        symbols = word.split()                                      # symbols = [\"l\", \"o\", \"w\", \"</w>\"], ...\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[symbols[i], symbols[i+1]] += freq                 # pairs = {(\"l\", \"o\") : 7, (\"o\", \"w\") : 7, ...}\n",
        "    \n",
        "    print(\"현재 pair들의 빈도수 =\", dict(pairs))\n",
        "    return pairs\n",
        "print()\n",
        "\n",
        "def merge_dict(pair, v_in):                 # pair는 best, v_in은 dictionary\n",
        "    print(f\"pair = {pair}\")\n",
        "    print(f\"v_in = {v_in}\")\n",
        "\n",
        "    v_out = {}\n",
        "    bigram = re.escape(\" \".join(pair))      # bigram = \"문자\\ 문자\"의 형태, re.escape는 입력된 문자열에 대해 특수문자들을 escape 처리함\n",
        "    print(f\"bigram = {bigram}\")             # re.escape = https://greeksharifa.github.io/%EC%A0%95%EA%B7%9C%ED%91%9C%ED%98%84%EC%8B%9D(re)/2018/08/24/regex-usage-09-other-functions/\n",
        "    new_pair = re.compile(r\"(?<!\\$)\" + bigram + r\"(?!\\$)\")\n",
        "\n",
        "    for word in v_in:\n",
        "        print(f\"word = {word}\")\n",
        "        w_out = new_pair.sub(\"\".join(pair), word)       # word에 있는 문자열 중에 p=re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')와 match 되는 문자열은 ''.join(pair)로 바꿈\n",
        "        print(f\"word_out = {w_out}\")\n",
        "        v_out[w_out] = v_in[word]\n",
        "\n",
        "    return v_out\n",
        "print()\n",
        "\n",
        "bpe_codes = {}\n",
        "bpe_codes_reverse = {}\n",
        "for i in range(num_merges):\n",
        "    display(Markdown(\"### Iteration {}\".format(i + 1)))\n",
        "    pairs = get_stats(dictionary)\n",
        "    best = max(pairs, key=pairs.get)                    # get() => dictionary에서 key로부터 value를 받아오는 함수\n",
        "    dictionary = merge_dict(best, dictionary)\n",
        "\n",
        "    bpe_codes[best] = i\n",
        "    bpe_codes_reverse[best[0] + best[1]] = best\n",
        "\n",
        "    print(\"New Merge = {}\".format(best))\n",
        "    print(\"Updated Dictionary = {}\".format(dictionary))\n",
        "print()\n",
        "\n",
        "print(bpe_codes)\n",
        "print(bpe_codes_reverse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HBzOUZZZIY5x",
        "outputId": "7954b795-c6f2-4fd7-b3de-f90f46ad391a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Iteration 1"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 = {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 8, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('e', 's'): 9, ('s', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3}\n",
            "pair = ('e', 's')\n",
            "v_in = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
            "bigram = e\\ s\n",
            "word = l o w </w>\n",
            "word_out = l o w </w>\n",
            "word = l o w e r </w>\n",
            "word_out = l o w e r </w>\n",
            "word = n e w e s t </w>\n",
            "word_out = n e w es t </w>\n",
            "word = w i d e s t </w>\n",
            "word_out = w i d es t </w>\n",
            "New Merge = ('e', 's')\n",
            "Updated Dictionary = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Iteration 2"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 = {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'es'): 6, ('es', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'es'): 3}\n",
            "pair = ('es', 't')\n",
            "v_in = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n",
            "bigram = es\\ t\n",
            "word = l o w </w>\n",
            "word_out = l o w </w>\n",
            "word = l o w e r </w>\n",
            "word_out = l o w e r </w>\n",
            "word = n e w es t </w>\n",
            "word_out = n e w est </w>\n",
            "word = w i d es t </w>\n",
            "word_out = w i d est </w>\n",
            "New Merge = ('es', 't')\n",
            "Updated Dictionary = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Iteration 3"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 = {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est'): 6, ('est', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est'): 3}\n",
            "pair = ('est', '</w>')\n",
            "v_in = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n",
            "bigram = est\\ </w>\n",
            "word = l o w </w>\n",
            "word_out = l o w </w>\n",
            "word = l o w e r </w>\n",
            "word_out = l o w e r </w>\n",
            "word = n e w est </w>\n",
            "word_out = n e w est</w>\n",
            "word = w i d est </w>\n",
            "word_out = w i d est</w>\n",
            "New Merge = ('est', '</w>')\n",
            "Updated Dictionary = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Iteration 4"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 = {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
            "pair = ('l', 'o')\n",
            "v_in = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "bigram = l\\ o\n",
            "word = l o w </w>\n",
            "word_out = lo w </w>\n",
            "word = l o w e r </w>\n",
            "word_out = lo w e r </w>\n",
            "word = n e w est</w>\n",
            "word_out = n e w est</w>\n",
            "word = w i d est</w>\n",
            "word_out = w i d est</w>\n",
            "New Merge = ('l', 'o')\n",
            "Updated Dictionary = {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Iteration 5"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 = {('lo', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
            "pair = ('lo', 'w')\n",
            "v_in = {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "bigram = lo\\ w\n",
            "word = lo w </w>\n",
            "word_out = low </w>\n",
            "word = lo w e r </w>\n",
            "word_out = low e r </w>\n",
            "word = n e w est</w>\n",
            "word_out = n e w est</w>\n",
            "word = w i d est</w>\n",
            "word_out = w i d est</w>\n",
            "New Merge = ('lo', 'w')\n",
            "Updated Dictionary = {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Iteration 6"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 = {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
            "pair = ('n', 'e')\n",
            "v_in = {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "bigram = n\\ e\n",
            "word = low </w>\n",
            "word_out = low </w>\n",
            "word = low e r </w>\n",
            "word_out = low e r </w>\n",
            "word = n e w est</w>\n",
            "word_out = ne w est</w>\n",
            "word = w i d est</w>\n",
            "word_out = w i d est</w>\n",
            "New Merge = ('n', 'e')\n",
            "Updated Dictionary = {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Iteration 7"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 = {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('ne', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
            "pair = ('ne', 'w')\n",
            "v_in = {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n",
            "bigram = ne\\ w\n",
            "word = low </w>\n",
            "word_out = low </w>\n",
            "word = low e r </w>\n",
            "word_out = low e r </w>\n",
            "word = ne w est</w>\n",
            "word_out = new est</w>\n",
            "word = w i d est</w>\n",
            "word_out = w i d est</w>\n",
            "New Merge = ('ne', 'w')\n",
            "Updated Dictionary = {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Iteration 8"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 = {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('new', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
            "pair = ('new', 'est</w>')\n",
            "v_in = {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n",
            "bigram = new\\ est</w>\n",
            "word = low </w>\n",
            "word_out = low </w>\n",
            "word = low e r </w>\n",
            "word_out = low e r </w>\n",
            "word = new est</w>\n",
            "word_out = newest</w>\n",
            "word = w i d est</w>\n",
            "word_out = w i d est</w>\n",
            "New Merge = ('new', 'est</w>')\n",
            "Updated Dictionary = {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Iteration 9"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 = {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
            "pair = ('low', '</w>')\n",
            "v_in = {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
            "bigram = low\\ </w>\n",
            "word = low </w>\n",
            "word_out = low</w>\n",
            "word = low e r </w>\n",
            "word_out = low e r </w>\n",
            "word = newest</w>\n",
            "word_out = newest</w>\n",
            "word = w i d est</w>\n",
            "word_out = w i d est</w>\n",
            "New Merge = ('low', '</w>')\n",
            "Updated Dictionary = {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Iteration 10"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 pair들의 빈도수 = {('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
            "pair = ('w', 'i')\n",
            "v_in = {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
            "bigram = w\\ i\n",
            "word = low</w>\n",
            "word_out = low</w>\n",
            "word = low e r </w>\n",
            "word_out = low e r </w>\n",
            "word = newest</w>\n",
            "word_out = newest</w>\n",
            "word = w i d est</w>\n",
            "word_out = wi d est</w>\n",
            "New Merge = ('w', 'i')\n",
            "Updated Dictionary = {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n",
            "\n",
            "{('e', 's'): 0, ('es', 't'): 1, ('est', '</w>'): 2, ('l', 'o'): 3, ('lo', 'w'): 4, ('n', 'e'): 5, ('ne', 'w'): 6, ('new', 'est</w>'): 7, ('low', '</w>'): 8, ('w', 'i'): 9}\n",
            "{'es': ('e', 's'), 'est': ('es', 't'), 'est</w>': ('est', '</w>'), 'lo': ('l', 'o'), 'low': ('lo', 'w'), 'ne': ('n', 'e'), 'new': ('ne', 'w'), 'newest</w>': ('new', 'est</w>'), 'low</w>': ('low', '</w>'), 'wi': ('w', 'i')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pairs(word):                        # 단어의 symbol pair 집합 반환, 단어는 변수 길이 문자열인 symbol의 tuple 형식\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    \n",
        "    return pairs\n",
        "\n",
        "def encode(origin):\n",
        "    word = tuple(origin) + (\"</w>\",)\n",
        "    display(Markdown(\"__word split into characters :__ <tt>{}</tt>\".format(word)))\n",
        "\n",
        "    pairs = get_pairs(word)\n",
        "\n",
        "    if not pairs:\n",
        "        return origin\n",
        "    \n",
        "    iter = 0\n",
        "    while True:\n",
        "        iter += 1\n",
        "        display(Markdown(\"__Iteration {} :__\".format(iter)))\n",
        "\n",
        "        print(f\"bigrams in the word = {pairs}\")\n",
        "        bigram = min(pairs, key=lambda pair: bpe_codes.get(pair, float(\"inf\")))\n",
        "        print(f\"candidates for merge = {bigram}\")\n",
        "\n",
        "        if bigram not in bpe_codes:\n",
        "            display(Markdown(\"__Candidate is not in BPE merges, algorithm stops.__\"))\n",
        "            break\n",
        "        \n",
        "        first, second = bigram\n",
        "        new_word = []\n",
        "        a_iter = 0\n",
        "        while a_iter < len(word):\n",
        "            try:\n",
        "                b_iter = word.index(first, a_iter)                          # tuple.index(element, start_index, end_index) => https://www.programiz.com/python-programming/methods/tuple/index\n",
        "                new_word.extend(word[a_iter:b_iter])                        # start_index (optional) - start scanning the element from the start_index\n",
        "                print(f\"word = {word}, new_word = {new_word}\")              # end_index (optional) - stop scanning the element at the end_index\n",
        "                a_iter = b_iter\n",
        "            except:\n",
        "                new_word.extend(word[a_iter:])\n",
        "                print(f\"EXCEPTION   word = {word}, new_word = {new_word}\")\n",
        "                break\n",
        "            \n",
        "            if word[a_iter] == first and a_iter < len(word) - 1 and word[a_iter+1] == second:\n",
        "                new_word.append(first + second)\n",
        "                print(f\"Merge {first} and {second}   word = {word}, new_word = {new_word}\")\n",
        "                a_iter += 2\n",
        "            else:\n",
        "                new_word.append(word[a_iter])\n",
        "                print(f\"Not Merge   word = {word}, new_word = {new_word}\")\n",
        "                a_iter += 1\n",
        "        \n",
        "        new_word = tuple(new_word)\n",
        "        word = new_word\n",
        "        print(f\"Word After Merging = {word}\")\n",
        "\n",
        "        if len(word) == 1:\n",
        "            print(\"Word Length is 1, BREAK\")\n",
        "            break\n",
        "        else:\n",
        "            pairs = get_pairs(word)\n",
        "    \n",
        "    if word[-1] == \"</w>\":\n",
        "        word = word[:-1]\n",
        "    elif word[-1].endswith(\"</w>\"):\n",
        "        word = word[:-1] + (word[-1].replace(\"</w>\", \"\"),)\n",
        "    \n",
        "    return word"
      ],
      "metadata": {
        "id": "gA1vTfvfUjKL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode(\"loki\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "fuQ8tOhFYWQ_",
        "outputId": "5b868233-ba7d-4b77-864d-fec8a93391f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__word split into characters :__ <tt>('l', 'o', 'k', 'i', '</w>')</tt>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__Iteration 1 :__"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word = {('o', 'k'), ('i', '</w>'), ('l', 'o'), ('k', 'i')}\n",
            "candidates for merge = ('l', 'o')\n",
            "word = ('l', 'o', 'k', 'i', '</w>'), new_word = []\n",
            "Merge l and o   word = ('l', 'o', 'k', 'i', '</w>'), new_word = ['lo']\n",
            "EXCEPTION   word = ('l', 'o', 'k', 'i', '</w>'), new_word = ['lo', 'k', 'i', '</w>']\n",
            "Word After Merging = ('lo', 'k', 'i', '</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__Iteration 2 :__"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word = {('lo', 'k'), ('i', '</w>'), ('k', 'i')}\n",
            "candidates for merge = ('lo', 'k')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__Candidate is not in BPE merges, algorithm stops.__"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lo', 'k', 'i')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encode(\"lowest\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "Pd_URMibYZTW",
        "outputId": "6ac107e3-3f0a-46d3-cf6e-c729c538f6bb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__word split into characters :__ <tt>('l', 'o', 'w', 'e', 's', 't', '</w>')</tt>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__Iteration 1 :__"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word = {('t', '</w>'), ('e', 's'), ('w', 'e'), ('o', 'w'), ('s', 't'), ('l', 'o')}\n",
            "candidates for merge = ('e', 's')\n",
            "word = ('l', 'o', 'w', 'e', 's', 't', '</w>'), new_word = ['l', 'o', 'w']\n",
            "Merge e and s   word = ('l', 'o', 'w', 'e', 's', 't', '</w>'), new_word = ['l', 'o', 'w', 'es']\n",
            "EXCEPTION   word = ('l', 'o', 'w', 'e', 's', 't', '</w>'), new_word = ['l', 'o', 'w', 'es', 't', '</w>']\n",
            "Word After Merging = ('l', 'o', 'w', 'es', 't', '</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__Iteration 2 :__"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word = {('t', '</w>'), ('es', 't'), ('w', 'es'), ('o', 'w'), ('l', 'o')}\n",
            "candidates for merge = ('es', 't')\n",
            "word = ('l', 'o', 'w', 'es', 't', '</w>'), new_word = ['l', 'o', 'w']\n",
            "Merge es and t   word = ('l', 'o', 'w', 'es', 't', '</w>'), new_word = ['l', 'o', 'w', 'est']\n",
            "EXCEPTION   word = ('l', 'o', 'w', 'es', 't', '</w>'), new_word = ['l', 'o', 'w', 'est', '</w>']\n",
            "Word After Merging = ('l', 'o', 'w', 'est', '</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__Iteration 3 :__"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word = {('w', 'est'), ('o', 'w'), ('est', '</w>'), ('l', 'o')}\n",
            "candidates for merge = ('est', '</w>')\n",
            "word = ('l', 'o', 'w', 'est', '</w>'), new_word = ['l', 'o', 'w']\n",
            "Merge est and </w>   word = ('l', 'o', 'w', 'est', '</w>'), new_word = ['l', 'o', 'w', 'est</w>']\n",
            "Word After Merging = ('l', 'o', 'w', 'est</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__Iteration 4 :__"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word = {('o', 'w'), ('l', 'o'), ('w', 'est</w>')}\n",
            "candidates for merge = ('l', 'o')\n",
            "word = ('l', 'o', 'w', 'est</w>'), new_word = []\n",
            "Merge l and o   word = ('l', 'o', 'w', 'est</w>'), new_word = ['lo']\n",
            "EXCEPTION   word = ('l', 'o', 'w', 'est</w>'), new_word = ['lo', 'w', 'est</w>']\n",
            "Word After Merging = ('lo', 'w', 'est</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__Iteration 5 :__"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word = {('lo', 'w'), ('w', 'est</w>')}\n",
            "candidates for merge = ('lo', 'w')\n",
            "word = ('lo', 'w', 'est</w>'), new_word = []\n",
            "Merge lo and w   word = ('lo', 'w', 'est</w>'), new_word = ['low']\n",
            "EXCEPTION   word = ('lo', 'w', 'est</w>'), new_word = ['low', 'est</w>']\n",
            "Word After Merging = ('low', 'est</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__Iteration 6 :__"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word = {('low', 'est</w>')}\n",
            "candidates for merge = ('low', 'est</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__Candidate is not in BPE merges, algorithm stops.__"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('low', 'est')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encode(\"lowing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "GY8i0sBoYbLl",
        "outputId": "b19e5bd5-196f-485e-d408-6f2085f6d81c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__word split into characters :__ <tt>('l', 'o', 'w', 'i', 'n', 'g', '</w>')</tt>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__Iteration 1 :__"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word = {('n', 'g'), ('i', 'n'), ('g', '</w>'), ('o', 'w'), ('w', 'i'), ('l', 'o')}\n",
            "candidates for merge = ('l', 'o')\n",
            "word = ('l', 'o', 'w', 'i', 'n', 'g', '</w>'), new_word = []\n",
            "Merge l and o   word = ('l', 'o', 'w', 'i', 'n', 'g', '</w>'), new_word = ['lo']\n",
            "EXCEPTION   word = ('l', 'o', 'w', 'i', 'n', 'g', '</w>'), new_word = ['lo', 'w', 'i', 'n', 'g', '</w>']\n",
            "Word After Merging = ('lo', 'w', 'i', 'n', 'g', '</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__Iteration 2 :__"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word = {('lo', 'w'), ('n', 'g'), ('i', 'n'), ('g', '</w>'), ('w', 'i')}\n",
            "candidates for merge = ('lo', 'w')\n",
            "word = ('lo', 'w', 'i', 'n', 'g', '</w>'), new_word = []\n",
            "Merge lo and w   word = ('lo', 'w', 'i', 'n', 'g', '</w>'), new_word = ['low']\n",
            "EXCEPTION   word = ('lo', 'w', 'i', 'n', 'g', '</w>'), new_word = ['low', 'i', 'n', 'g', '</w>']\n",
            "Word After Merging = ('low', 'i', 'n', 'g', '</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__Iteration 3 :__"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word = {('g', '</w>'), ('low', 'i'), ('n', 'g'), ('i', 'n')}\n",
            "candidates for merge = ('g', '</w>')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__Candidate is not in BPE merges, algorithm stops.__"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('low', 'i', 'n', 'g')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encode(\"highing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "S4aZ3CDCYbhP",
        "outputId": "474f1898-fb3b-4c94-f01d-fe085618eb86"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__word split into characters :__ <tt>('h', 'i', 'g', 'h', 'i', 'n', 'g', '</w>')</tt>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__Iteration 1 :__"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams in the word = {('n', 'g'), ('i', 'n'), ('i', 'g'), ('g', '</w>'), ('g', 'h'), ('h', 'i')}\n",
            "candidates for merge = ('n', 'g')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "__Candidate is not in BPE merges, algorithm stops.__"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('h', 'i', 'g', 'h', 'i', 'n', 'g')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SentencePiece in NLP, 자연어 처리에서의 SentencePiece**  \n",
        "---\n",
        "\n",
        "##### **SPM의 SentencePieceTrainer의 Arguments**  \n",
        "- input : train file\n",
        "- model_prefix : 만들 모델의 이름\n",
        "- vocab_size : 단어 집합의 크기\n",
        "- model_type : 사용할 모델, default는 unigram이며 bpe, char, word 등이 가능\n",
        "- max_sentence_length : 문장의 최대 길이\n",
        "- pad_id, pad_piece : PAD token id와 value\n",
        "- unk_id, unk_piece : Unkown token id와 value\n",
        "- bos_id, bos_piece : Begion of Sentence token id와 value\n",
        "- eos_id, eos_piece : End of Sentence token id와 value\n",
        "- user_defined_symbols : 사용자 정의 token  \n",
        "  \n",
        "vocab 생성이 완료되면 **imdb.model과 imdb.vocab 파일**이 생성됨. **vocab 파일에서 학습된 subword들 확인 가능**"
      ],
      "metadata": {
        "id": "Pw4wH-80Tc2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMDB Review**"
      ],
      "metadata": {
        "id": "V4l18UMSIrM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")\n",
        "train_df = pd.read_csv(\"IMDb_Reviews.csv\")\n",
        "print(\"Train Data Length =\", len(train_df))\n",
        "print()\n",
        "\n",
        "with open(\"IMDb_Reviews.txt\", \"w\", encoding=\"utf8\") as file:\n",
        "    file.write(\"\\n\".join(train_df[\"review\"]))\n",
        "\n",
        "spm.SentencePieceTrainer.Train('--input=IMDb_Reviews.txt --model_prefix=imdb --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')\n",
        "\n",
        "vocab_list = pd.read_csv(\"imdb.vocab\", sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
        "vocab_list.sample(10)\n",
        "print(len(vocab_list))\n",
        "print()\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "vocab_file = \"imdb.model\"\n",
        "sp.load(vocab_file)\n",
        "print()\n",
        "\n",
        "lines = [\"I didn't at all think of it this way.\", \"I have waited a long time for someone to film\"]\n",
        "for line in lines:\n",
        "    print(line)\n",
        "    print(sp.encode_as_pieces(line))        # 입력된 문장에 대해 subword sequence로 변환\n",
        "    print(sp.encode_as_ids(line))           # 입력된 문장에 대해 integer sequence로 변환\n",
        "    print()\n",
        "\n",
        "print(\"Get Piece Size =\", sp.GetPieceSize)                                                                                                      # 단어 집합의 크기\n",
        "print(\"ID to Piece =\", sp.IdToPiece(430))                                                                                                       # 정수 => subword 변환\n",
        "print(\"Piece To ID =\", sp.PieceToId(\"_character\"))                                                                                              # subword => 정수 변환\n",
        "print(\"Decode IDs =\", sp.DecodeIds([41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91]))                                                       # 정수 seq => 문장 변환\n",
        "print(\"Decode Pieces =\", sp.DecodePieces(['▁I', '▁have', '▁wa', 'ited', '▁a', '▁long', '▁time', '▁for', '▁someone', '▁to', '▁film']))        # subword seq => 문장 변환\n",
        "print(\"Encode to Integer Seq =\", sp.encode('I have waited a long time for someone to film', out_type=int))                                      # 문장 => 정수 seq 변환\n",
        "print(\"Encode to Subword Seq =\", sp.encode('I have waited a long time for someone to film', out_type=str))                                      # 문장 => subword seq 변환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DnqDkWcInx4",
        "outputId": "2c330913-da8c-4d2d-d34a-055de46cfd03"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Length = 50000\n",
            "\n",
            "5000\n",
            "\n",
            "\n",
            "I didn't at all think of it this way.\n",
            "['▁I', '▁didn', \"'\", 't', '▁at', '▁all', '▁think', '▁of', '▁it', '▁this', '▁way', '.']\n",
            "[41, 623, 4950, 4926, 138, 169, 378, 30, 58, 73, 413, 4945]\n",
            "\n",
            "I have waited a long time for someone to film\n",
            "['▁I', '▁have', '▁wa', 'ited', '▁a', '▁long', '▁time', '▁for', '▁someone', '▁to', '▁film']\n",
            "[41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91]\n",
            "\n",
            "Get Piece Size = <bound method SentencePieceProcessor.GetPieceSize of <sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x7f6107597f60> >>\n",
            "ID to Piece = ▁character\n",
            "Piece To ID = 0\n",
            "Decode IDs = I have waited a long time for someone to film\n",
            "Decode Pieces = I have waited a long time for someone to film\n",
            "Encode to Integer Seq = [41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91]\n",
            "Encode to Subword Seq = ['▁I', '▁have', '▁wa', 'ited', '▁a', '▁long', '▁time', '▁for', '▁someone', '▁to', '▁film']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Naver Movie Review**"
      ],
      "metadata": {
        "id": "EVlGvrvrIw-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
        "naver_df = pd.read_table('ratings.txt')\n",
        "\n",
        "naver_df = naver_df.dropna(how=\"any\")           # 결측치 제거\n",
        "print('Length :',len(naver_df))\n",
        "print()\n",
        "\n",
        "with open(\"naver_review.txt\", \"w\", encoding=\"utf8\") as file:\n",
        "    file.write(\"\\n\".join(naver_df[\"document\"]))\n",
        "\n",
        "spm.SentencePieceTrainer.Train(\"--input=naver_review.txt --model_prefix=naver --vocab_size=5000 --model_type=bpe --max_sentence_length=9999\")\n",
        "vocab_list = pd.read_csv('naver.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
        "len(vocab_list)\n",
        "print()\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "vocab_file = \"naver.model\"\n",
        "sp.load(vocab_file)\n",
        "\n",
        "lines = [\"뭐 이딴 것도 영화냐.\", \"진짜 최고의 영화입니다 ㅋㅋ\"]\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  print(sp.encode_as_pieces(line))\n",
        "  print(sp.encode_as_ids(line))\n",
        "  print()\n",
        "\n",
        "print(\"Get Piece Size =\", sp.GetPieceSize)                                                          # 단어 집합의 크기\n",
        "print(\"ID to Piece =\", sp.IdToPiece(4))                                                             # 정수 => subword 변환\n",
        "print(\"Piece To ID =\", sp.PieceToId(\"영화\"))                                                        # subword => 정수 변환\n",
        "print(\"Decode IDs =\", sp.DecodeIds([54, 200, 821, 85]))                                             # 정수 seq => 문장 변환\n",
        "print(\"Decode Pieces =\", sp.DecodePieces(['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']))            # subword seq => 문장 변환\n",
        "print(\"Encode to Integer Seq =\", sp.encode('진짜 최고의 영화입니다 ㅋㅋ', out_type=int))            # 문장 => 정수 seq 변환\n",
        "print(\"Encode to Subword Seq =\", sp.encode('진짜 최고의 영화입니다 ㅋㅋ', out_type=str))            # 문장 => subword seq 변환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-g_t2rzIw-C",
        "outputId": "3d792289-1d7f-486f-feec-ff8ec8729f2e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length : 199992\n",
            "\n",
            "\n",
            "뭐 이딴 것도 영화냐.\n",
            "['▁뭐', '▁이딴', '▁것도', '▁영화냐', '.']\n",
            "[132, 966, 1296, 2590, 3276]\n",
            "\n",
            "진짜 최고의 영화입니다 ㅋㅋ\n",
            "['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']\n",
            "[54, 200, 821, 85]\n",
            "\n",
            "Get Piece Size = <bound method SentencePieceProcessor.GetPieceSize of <sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x7f6107b8e2d0> >>\n",
            "ID to Piece = 영화\n",
            "Piece To ID = 4\n",
            "Decode IDs = 진짜 최고의 영화입니다 ᄏᄏ\n",
            "Decode Pieces = 진짜 최고의 영화입니다 ᄏᄏ\n",
            "Encode to Integer Seq = [54, 200, 821, 85]\n",
            "Encode to Subword Seq = ['▁진짜', '▁최고의', '▁영화입니다', '▁ᄏᄏ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SubwordTextEncoder in NLP, 자연어 처리에서의 SubwordTextEncoder**"
      ],
      "metadata": {
        "id": "PwLDRaCnI5Tk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenizing IMDB Review**"
      ],
      "metadata": {
        "id": "zMyoaLOSI5Tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")\n",
        "train_df = pd.read_csv('IMDb_Reviews.csv')\n",
        "\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(train_df['review'], target_vocab_size=2 ** 13)    # subword로 이루어진 단어 집합 & Integer Encoding 수행\n",
        "print(train_df['review'][20])                                                               # 리뷰 sample\n",
        "print('Tokenized sample question = {}'.format(tokenizer.encode(train_df['review'][20])))    # Encoding된 리뷰 sample 확인\n",
        "print()\n",
        "\n",
        "sample_str = \"It's mind-blowing to me that this film was even made.\"\n",
        "encoded_str = tokenizer.encode(sample_str)          # Encoding 수행\n",
        "print(f\"Encoded Sentence = {encoded_str}\")\n",
        "decoded_str = tokenizer.decode(encoded_str)         # Decoding 수행\n",
        "print(f\"Decoded Sentence = {decoded_str}\")\n",
        "print()\n",
        "\n",
        "print(\"단어 집합의 크기 =\", tokenizer.vocab_size)\n",
        "for token_str in encoded_str:\n",
        "    print(f\"{token_str} ======> {tokenizer.decode([token_str])}\")\n",
        "\n",
        "sample_str = \"It's mind-blowing to me that this film was evenxyz made.\"            # even이라는 단어에 임의의 xyz 문자를 추가하여 subword 분리 작업 확인\n",
        "encoded_str = tokenizer.encode(sample_str)          # Encoding 수행\n",
        "print(f\"Encoded Sentence = {encoded_str}\")\n",
        "decoded_str = tokenizer.decode(encoded_str)         # Decoding 수행\n",
        "print(f\"Decoded Sentence = {decoded_str}\")\n",
        "print()\n",
        "\n",
        "for token_str in encoded_str:\n",
        "    print(f\"{token_str} ======> {tokenizer.decode([token_str])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rGN8TFbI5Tp",
        "outputId": "c94bda79-68c8-41f9-8336-0fe7ed55d820"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretty bad PRC cheapie which I rarely bother to watch over again, and it's no wonder -- it's slow and creaky and dull as a butter knife. Mad doctor George Zucco is at it again, turning a dimwitted farmhand in overalls (Glenn Strange) into a wolf-man. Unfortunately, the makeup is virtually non-existent, consisting only of a beard and dimestore fangs for the most part. If it were not for Zucco and Strange's presence, along with the cute Anne Nagel, this would be completely unwatchable. Strange, who would go on to play Frankenstein's monster for Unuiversal in two years, does a Lenny impression from \"Of Mice and Men\", it seems.<br /><br />*1/2 (of Four)\n",
            "Tokenized sample question = [1590, 4162, 132, 7107, 1892, 2983, 578, 76, 12, 4632, 3422, 7, 160, 175, 372, 2, 5, 39, 8051, 8, 84, 2652, 497, 39, 8051, 8, 1374, 5, 3461, 2012, 48, 5, 2263, 21, 4, 2992, 127, 4729, 711, 3, 1391, 8044, 3557, 1277, 8102, 2154, 5681, 9, 42, 15, 372, 2, 3773, 4, 3502, 2308, 467, 4890, 1503, 11, 3347, 1419, 8127, 29, 5539, 98, 6099, 58, 94, 4, 1388, 4230, 8057, 213, 3, 1966, 2, 1, 6700, 8044, 9, 7069, 716, 8057, 6600, 2, 4102, 36, 78, 6, 4, 1865, 40, 5, 3502, 1043, 1645, 8044, 1000, 1813, 23, 1, 105, 1128, 3, 156, 15, 85, 33, 23, 8102, 2154, 5681, 5, 6099, 8051, 8, 7271, 1055, 2, 534, 22, 1, 3046, 5214, 810, 634, 8120, 2, 14, 71, 34, 436, 3311, 5447, 783, 3, 6099, 2, 46, 71, 193, 25, 7, 428, 2274, 2260, 6487, 8051, 8, 2149, 23, 1138, 4117, 6023, 163, 11, 148, 735, 2, 164, 4, 5277, 921, 3395, 1262, 37, 639, 1349, 349, 5, 2460, 328, 15, 5349, 8127, 24, 10, 16, 10, 17, 8054, 8061, 8059, 8062, 29, 6, 6607, 8126, 8053]\n",
            "\n",
            "Encoded Sentence = [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 79, 681, 8058]\n",
            "Decoded Sentence = It's mind-blowing to me that this film was even made.\n",
            "\n",
            "단어 집합의 크기 = 8268\n",
            "137 ======> It\n",
            "8051 ======> '\n",
            "8 ======> s \n",
            "910 ======> mind\n",
            "8057 ======> -\n",
            "2169 ======> blow\n",
            "36 ======> ing \n",
            "7 ======> to \n",
            "103 ======> me \n",
            "13 ======> that \n",
            "14 ======> this \n",
            "32 ======> film \n",
            "18 ======> was \n",
            "79 ======> even \n",
            "681 ======> made\n",
            "8058 ======> .\n",
            "Encoded Sentence = [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 7974, 8132, 8133, 997, 681, 8058]\n",
            "Decoded Sentence = It's mind-blowing to me that this film was evenxyz made.\n",
            "\n",
            "137 ======> It\n",
            "8051 ======> '\n",
            "8 ======> s \n",
            "910 ======> mind\n",
            "8057 ======> -\n",
            "2169 ======> blow\n",
            "36 ======> ing \n",
            "7 ======> to \n",
            "103 ======> me \n",
            "13 ======> that \n",
            "14 ======> this \n",
            "32 ======> film \n",
            "18 ======> was \n",
            "7974 ======> even\n",
            "8132 ======> x\n",
            "8133 ======> y\n",
            "997 ======> z \n",
            "681 ======> made\n",
            "8058 ======> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenizing Naver Movie Review**"
      ],
      "metadata": {
        "id": "uKzhMjUaI5Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
        "\n",
        "train_data = pd.read_table('ratings_train.txt')\n",
        "test_data = pd.read_table('ratings_test.txt')\n",
        "\n",
        "train_data = train_data.dropna(how=\"any\")\n",
        "\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(train_data['document'], target_vocab_size=2 ** 13)\n",
        "print(train_data['document'][20])\n",
        "print('Tokenized sample question: {}'.format(tokenizer.encode(train_data['document'][20])))\n",
        "\n",
        "sample_str = train_data[\"document\"][21]\n",
        "encoded_str = tokenizer.encode(sample_str)          # Encoding 수행\n",
        "print(f\"Encoded Sentence = {encoded_str}\")\n",
        "decoded_str = tokenizer.decode(encoded_str)         # Decoding 수행\n",
        "print(f\"Decoded Sentence = {decoded_str}\")\n",
        "print()\n",
        "assert decoded_str == sample_str\n",
        "\n",
        "for token_str in encoded_str:\n",
        "    print(f\"{token_str} ======> {tokenizer.decode([token_str])}\")\n",
        "\n",
        "sample_str = \"이 영화 굉장히 재밌다 킄핫핫ㅎ\"\n",
        "encoded_str = tokenizer.encode(sample_str)          # Encoding 수행\n",
        "print(f\"Encoded Sentence = {encoded_str}\")\n",
        "decoded_str = tokenizer.decode(encoded_str)         # Decoding 수행\n",
        "print(f\"Decoded Sentence = {decoded_str}\")\n",
        "print()\n",
        "assert decoded_str == sample_str\n",
        "\n",
        "for token_str in encoded_str:\n",
        "    print(f\"{token_str} ======> {tokenizer.decode([token_str])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBGFjpnvI5Tt",
        "outputId": "3cdd4741-2a5e-4f59-eff3-e07a6177d8ab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "나름 심오한 뜻도 있는 듯. 그냥 학생이 선생과 놀아나는 영화는 절대 아님\n",
            "Tokenized sample question: [669, 4700, 17, 1749, 8, 96, 131, 1, 48, 2239, 4, 7466, 32, 1274, 2655, 7, 80, 749, 1254]\n",
            "Encoded Sentence = [570, 892, 36, 584, 159, 7091, 201]\n",
            "Decoded Sentence = 보면서 웃지 않는 건 불가능하다\n",
            "\n",
            "570 ======> 보면서 \n",
            "892 ======> 웃\n",
            "36 ======> 지 \n",
            "584 ======> 않는 \n",
            "159 ======> 건 \n",
            "7091 ======> 불가능\n",
            "201 ======> 하다\n",
            "Encoded Sentence = [4, 23, 1364, 2157, 8235, 8128, 8130, 8235, 8147, 8169, 8235, 8147, 8169, 393]\n",
            "Decoded Sentence = 이 영화 굉장히 재밌다 킄핫핫ㅎ\n",
            "\n",
            "4 ======> 이 \n",
            "23 ======> 영화 \n",
            "1364 ======> 굉장히 \n",
            "2157 ======> 재밌다 \n",
            "8235 ======> �\n",
            "8128 ======> �\n",
            "8130 ======> �\n",
            "8235 ======> �\n",
            "8147 ======> �\n",
            "8169 ======> �\n",
            "8235 ======> �\n",
            "8147 ======> �\n",
            "8169 ======> �\n",
            "393 ======> ㅎ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **HuggingFace Tokenizer**  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**BertWordPieceTokenizer** 이외에 **ByteLevelBPETokenizer**, **CharBPETokenizer**, **SentencePieceBPETokenizer** 등이 존재\n",
        "\n",
        "* **BertWordPieceTokenizer :** BERT에서 사용된 워드피스 토크나이저(WordPiece Tokenizer)\n",
        "* **CharBPETokenizer :** 오리지널 BPE\n",
        "* **ByteLevelBPETokenizer :** BPE의 바이트 레벨 버전\n",
        "* **SentencePieceBPETokenizer :** 앞서 본 패키지 센텐스피스(SentencePiece)와 호환되는 BPE 구현체"
      ],
      "metadata": {
        "id": "F_dJ0W2GJLiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
        "\n",
        "naver_df = pd.read_table('ratings.txt')\n",
        "naver_df = naver_df.dropna(how = 'any')\n",
        "with open('naver_review.txt', 'w', encoding='utf8') as file:\n",
        "    file.write('\\n'.join(naver_df['document']))\n",
        "\n",
        "tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False)\n",
        "\n",
        "data_file = 'naver_review.txt'\n",
        "vocab_size = 30000\n",
        "limit_alphabet = 6000\n",
        "min_frequency = 5\n",
        "tokenizer.train(files=data_file, vocab_size=vocab_size, limit_alphabet=limit_alphabet, min_frequency=min_frequency)\n",
        "tokenizer.save(\"./data\")\n",
        "\n",
        "vocab_df = pd.read_fwf(\"./data\", header=None)\n",
        "encoded = tokenizer.encode(\"아 배고픈데 짜장면먹고싶다\")\n",
        "print(\"Tokenization Result =\", encoded.tokens)\n",
        "print(\"Integer Encoding =\", encoded.ids)                # Integer Encoding의 결과 => 실제 DL 모델에 Input으로 사용\n",
        "print(\"Decoding =\", tokenizer.decode(encoded.ids))\n",
        "print()\n",
        "\n",
        "encoded = tokenizer.encode(\"커피 한잔의 여유를 즐기다\")\n",
        "print(\"Tokenization Result =\", encoded.tokens)\n",
        "print(\"Integer Encoding =\", encoded.ids)                # Integer Encoding의 결과 => 실제 DL 모델에 Input으로 사용\n",
        "print(\"Decoding =\", tokenizer.decode(encoded.ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmEGbrIMJLi3",
        "outputId": "9d9a8585-88c1-4876-d7e1-48799fa66f6a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization Result = ['아', '배고', '##픈', '##데', '짜장면', '##먹고', '##싶다']\n",
            "Integer Encoding = [2111, 20630, 4044, 3299, 24681, 7871, 7379]\n",
            "Decoding = 아 배고픈데 짜장면먹고싶다\n",
            "\n",
            "Tokenization Result = ['커피', '한잔', '##의', '여유', '##를', '즐기', '##다']\n",
            "Integer Encoding = [12825, 25647, 3313, 12696, 3242, 10784, 3275]\n",
            "Decoding = 커피 한잔의 여유를 즐기다\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SentencePieceBPETokenizer()\n",
        "tokenizer.train('naver_review.txt', vocab_size=10000, min_frequency=5)\n",
        "\n",
        "encoded = tokenizer.encode(\"이 영화는 정말 재미있습니다.\")\n",
        "print(encoded.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oziDUl4tm1xO",
        "outputId": "f40e60ac-cea0-4dda-dd42-2b694a12ae27"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁이', '▁영화는', '▁정말', '▁재미있', '습니다.']\n"
          ]
        }
      ]
    }
  ]
}