{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kHEzCYyFX_st",
        "eX-baVppy47L",
        "QISHxS0bFqlG",
        "eDg0ENK0FvU9",
        "GpL1rJIvH9TV",
        "ztM5lerp3HmD",
        "duRfWJupO4UI",
        "7BXbLIx2mVfY",
        "7FlZDeuqmrWN",
        "fOR6MP6nydg0",
        "M5ahiNLmAFE-",
        "IAJDy-epAwsW",
        "M8Y6e-htBicO",
        "5UAdeCKPzsZp",
        "rO_H2S1PzzKr",
        "sIQs-xjRzzfM"
      ],
      "authorship_tag": "ABX9TyPfaoovUU7ntvwODDxmZNGO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kim-Jeong-Ju/AI_Modeling_NLP/blob/main/Word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Packages and Modules Importation**"
      ],
      "metadata": {
        "id": "kHEzCYyFX_st"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPcjN9FdlBQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be8f95d-324d-4bce-aa8a-fb5ab7b0c80b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.8/dist-packages (0.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: glove_python_binary in /usr/local/lib/python3.8/dist-packages (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from glove_python_binary) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from glove_python_binary) (1.7.3)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "import urllib.request\n",
        "from urllib.request import urlretrieve, urlopen\n",
        "import zipfile\n",
        "import gzip\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from lxml import etree\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import log\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences, skipgrams\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Embedding, Reshape, Activation, Input, Dot, Flatten\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import SVG\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "!pip install konlpy\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "!pip install glove_python_binary\n",
        "from glove import Corpus, Glove"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Embedding, 워드 임베딩**"
      ],
      "metadata": {
        "id": "JP0vS7XYZ1Ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **영어/한국어 Word2Vec Generation**  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "XML 형태의 Training Data를 전처리하여 **자연어로 변환**하는 작업 필요. `<content>` tag 내의 내용을 parsing하여 가져와야 함. Laughter와 Applause와 같은 배경음 역시 제거하는 전처리 작업 필요"
      ],
      "metadata": {
        "id": "eX-baVppy47L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **영어 Word2Vec Generation**"
      ],
      "metadata": {
        "id": "QISHxS0bFqlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")\n",
        "\n",
        "start_time = time.time()\n",
        "targetXML = open(\"ted_en-20160408.xml\", \"r\", encoding=\"UTF8\")\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = \"\\n\".join(target_text.xpath(\"//content/text()\"))\n",
        "\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)             # https://uipath.tistory.com/91 참조\n",
        "sent_text = sent_tokenize(content_text)                         # NLTK 문장 토큰화 수행\n",
        "\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "    normalized_text.append(tokens)\n",
        "\n",
        "result = [word_tokenize(sentence) for sentence in normalized_text]\n",
        "print('Total Sample Length = {}'.format(len(result)))\n",
        "print(\"Execution Time =\", time.time() - start_time)\n",
        "\n",
        "with open('ted_en-20160408_token.pkl', 'wb') as file:           # XML 데이터를 tokenize하는데 시간이 많이 필요하므로 토큰화된 list를 pickle 파일로 저장\n",
        "    pickle.dump(result, file)\n",
        "with open('ted_en-20160408_token.pkl', 'rb') as file:           # 저장한 XML 데이터에 대한 토큰화 pickle 파일 불러오기 \n",
        "    result = pickle.load(file)\n",
        "\n",
        "for line in result[:3]:                                         # 불러온 데이터의 sample 3개 출력\n",
        "    print(line)"
      ],
      "metadata": {
        "id": "QO2wa2tyZ6nm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fda3d1f-dbed-4a32-eb61-c29168bb8cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sample Length = 273380\n",
            "Execution Time = 4722.625554561615\n",
            "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
            "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
            "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2Vec의 arguments**  \n",
        "- **sentence :** 단어 -> 벡터로 변환할 문장 입력\n",
        "- **size :** 단어 벡터의 특징값, 임베딩된 벡터의 차원\n",
        "- **window :** context window의 크기\n",
        "- **min_count :** 단어 최소 빈도수 제한 -> 빈도수가 적은 단어들은 학습 X\n",
        "- **workers :** 학습을 위한 process 수\n",
        "- **sg :** 0이면 CBOW, 1이면 Skip-gram"
      ],
      "metadata": {
        "id": "m-QfD5CgCvhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)\n",
        "\n",
        "model_result = model.wv.most_similar(\"man\")\n",
        "print(model_result)\n",
        "print()\n",
        "\n",
        "model.wv.save_word2vec_format(\"ENG_w2v\")                        # 모델 저장\n",
        "loaded_model = KeyedVectors.load_word2vec_format(\"ENG_w2v\")     # 모델 로드\n",
        "loaded_result = loaded_model.most_similar(\"man\")\n",
        "print(loaded_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbi4TFIW2jxz",
        "outputId": "7734bba2-336f-44e5-cbc3-5816ada5bd74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.8404335379600525), ('guy', 0.8093708157539368), ('boy', 0.7701140642166138), ('lady', 0.7535463571548462), ('soldier', 0.7397874593734741), ('gentleman', 0.7388907670974731), ('girl', 0.735160231590271), ('kid', 0.6878196597099304), ('poet', 0.6819333434104919), ('smith', 0.6629151105880737)]\n",
            "\n",
            "[('woman', 0.8404335379600525), ('guy', 0.8093708157539368), ('boy', 0.7701140642166138), ('lady', 0.7535463571548462), ('soldier', 0.7397874593734741), ('gentleman', 0.7388907670974731), ('girl', 0.735160231590271), ('kid', 0.6878196597099304), ('poet', 0.6819333434104919), ('smith', 0.6629151105880737)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **한국어 Word2Vec Generation**"
      ],
      "metadata": {
        "id": "eDg0ENK0FvU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
        "train_data = pd.read_table(\"ratings.txt\")\n",
        "print(train_data.head())\n",
        "print(len(train_data))                          # 전체 영화 리뷰 size 출력\n",
        "print(train_data.isnull().values.any())         # NULL과 같은 결측치 확인\n",
        "print()\n",
        "\n",
        "train_data = train_data.dropna(how=\"any\")\n",
        "print(train_data.isnull().values.any())         # NULL과 같은 결측치 재확인\n",
        "print(len(train_data))                          # 전체 영화 리뷰 size 재출력\n",
        "print()\n",
        "\n",
        "train_data[\"document\"] = train_data[\"document\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
        "print(train_data.head())\n",
        "\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']     # 불용어 제거\n",
        "start_time = time.time()\n",
        "\n",
        "okt = Okt()\n",
        "tokenized_data = []\n",
        "for sent in tqdm(train_data[\"document\"], desc=\"Eliminating Stopwords...\"):\n",
        "    tokenized_sent = okt.morphs(sent, stem=True)\n",
        "    stopwords_removed_sent = [word for word in tokenized_sent if word not in stopwords]\n",
        "    tokenized_data.append(stopwords_removed_sent)\n",
        "print(\"Execution Time =\", time.time() - start_time)\n",
        "print()\n",
        "\n",
        "print(\"Max Length of Movie Review =\", max(len(review) for review in tokenized_data))\n",
        "print(\"Avg Length of Movie Review =\", sum(map(len, tokenized_data)) / len(tokenized_data))\n",
        "plt.hist([len(review) for review in tokenized_data], bins=50)\n",
        "plt.xlabel(\"Length of samples\")\n",
        "plt.ylabel(\"Number of samples\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        },
        "id": "U5URKqUh2yzB",
        "outputId": "92280d2b-f188-4c22-9c04-0aea85895f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         id                                           document  label\n",
            "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
            "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
            "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
            "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
            "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1\n",
            "200000\n",
            "True\n",
            "\n",
            "False\n",
            "199992\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-0c6cf18d3ef0>:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train_data[\"document\"] = train_data[\"document\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         id                                           document  label\n",
            "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
            "1   8132799  디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...      1\n",
            "2   4655635                   폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고      1\n",
            "3   9251303   와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지      1\n",
            "4  10067386                         안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화      1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Eliminating Stopwords...: 100%|██████████| 199992/199992 [22:56<00:00, 145.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time = 1379.6876459121704\n",
            "\n",
            "Max Length of Movie Review = 72\n",
            "Avg Length of Movie Review = 10.716703668146726\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences=tokenized_data, size=100, window=5, min_count=5, workers=4, sg=0)\n",
        "print(model.wv.vectors.shape)\n",
        "print(model.wv.most_similar(\"최민식\"))\n",
        "print(model.wv.most_similar(\"히어로\"))"
      ],
      "metadata": {
        "id": "O1iYWJabHqjY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cecf11f-6d35-4a6b-8f61-ddc08ee6b3d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16477, 100)\n",
            "[('한석규', 0.8837981224060059), ('김수현', 0.8669970631599426), ('최민수', 0.8655436635017395), ('안성기', 0.8640116453170776), ('이정재', 0.8534038066864014), ('임원희', 0.843616247177124), ('설경구', 0.8410159349441528), ('박중훈', 0.8410141468048096), ('엄태웅', 0.8379778861999512), ('유다인', 0.8361737132072449)]\n",
            "[('호러', 0.8738075494766235), ('슬래셔', 0.8629007339477539), ('느와르', 0.8564665913581848), ('무협', 0.8532429933547974), ('무비', 0.8410012722015381), ('블록버스터', 0.8282603025436401), ('물', 0.8112932443618774), ('멜로', 0.8098613023757935), ('물의', 0.8081122636795044), ('블랙', 0.786051869392395)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pretrained Word2Vec Embedding**  \n",
        "> 자연어 처리 작업을 할때, 케라스의 Embedding()를 사용하여 갖고 있는 훈련 데이터로부터 처음부터 임베딩 벡터를 훈련시키기도 하지만, 위키피디아 등의 방대한 데이터로 **사전에 훈련된 워드 임베딩(pre-trained word embedding vector)를 가지고 와서 해당 벡터들의 값을 원하는 작업에 사용**할 수도 있음  \n",
        ">  \n",
        "> 예를 들어서 감성 분류 작업을 하는데 훈련 데이터의 양이 부족한 상황이라면, **다른 방대한 데이터를 Word2Vec이나 GloVe 등으로 사전에 학습시켜놓은 임베딩 벡터들을 가지고 와서 모델의 입력으로 사용하는 것이 때로는 더 좋은 성능**을 얻을 수 있음  \n",
        ">  \n",
        "> 구글이 제공하는 **사전 훈련된(미리 학습되어져 있는) Word2Vec 모델**을 사용. 구글은 **사전 훈련된 3백만 개의 Word2Vec 단어 벡터**들을 제공함. **각 임베딩 벡터의 차원은 300임**"
      ],
      "metadata": {
        "id": "GpL1rJIvH9TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", filename=\"GoogleNews-vectors-negative300.bin.gz\")\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "metadata": {
        "id": "xSn9UTv7OATn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "9379a7da-caf2-463c-8ffc-9330a8d4e79f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-cd77ce909735>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GoogleNews-vectors-negative300.bin.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mword2vec_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    641\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Negative Samping을 활용한 Word2Vce Generation**"
      ],
      "metadata": {
        "id": "ztM5lerp3HmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Skip-Gram with Negative Sampling, SGNS**  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**중심 단어 - 주변 단어의 관계 성립을 위해 1개의 sample에 최소한 2개의 단어가 존재해야 함.** 최소 2개의 단어가 없다면 sample을 구성할 수 없어 error 발생함. 이를 만족하지 않는 sample들은 제거하는 작업 필요"
      ],
      "metadata": {
        "id": "duRfWJupO4UI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=(\"headers\", \"footers\", \"quotes\"))\n",
        "documents = dataset.data\n",
        "print(\"Total Sample Size =\", len(documents))\n",
        "print()\n",
        "\n",
        "news_df = pd.DataFrame({\"document\" : documents})\n",
        "news_df[\"clean_doc\"] = news_df[\"document\"].str.replace(\"[^a-zA-Z]\", \" \")                                                    # 특수문자 제거\n",
        "news_df[\"clean_doc\"] = news_df[\"clean_doc\"].apply(lambda x:\" \".join([word for word in x.split() if len(word) > 3]))         # 길이가 3 이하인 짧은 단어 제거\n",
        "news_df[\"clean_doc\"] = news_df[\"clean_doc\"].apply(lambda x: x.lower())                                                      # 소문자화\n",
        "print(news_df.head())\n",
        "print(news_df.isnull().values.any())            # 결측치 확인\n",
        "print()\n",
        "\n",
        "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
        "print(news_df.isnull().values.any())            # 결측치 재확인\n",
        "news_df.dropna(inplace=True)\n",
        "print(\"Total Sample Size =\", len(news_df))\n",
        "print()\n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words(\"english\")         # 불용어 제거\n",
        "tokenized_doc = news_df[\"clean_doc\"].apply(lambda x: x.split())\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "tokenized_doc = tokenized_doc.to_list()\n",
        "print(np.shape(tokenized_doc))\n",
        "print()\n",
        "\n",
        "drop_train = [index for index, sent in enumerate(tokenized_doc) if len(sent) <= 1]      # 단어가 1개 이하인 sample을 찾아 제거\n",
        "tokenized_doc = np.delete(tokenized_doc, drop_train, axis=0)\n",
        "print(\"Total Sample Size =\", len(tokenized_doc))\n",
        "print()\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_doc)\n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = {value : key for key, value in word2idx.items()}\n",
        "encoded = tokenizer.texts_to_sequences(tokenized_doc)\n",
        "print(encoded[:2])\n",
        "print()\n",
        "\n",
        "vocab_size = len(word2idx) + 1\n",
        "print(\"Word Vocabulary Size =\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21ngqS0I3LBR",
        "outputId": "60b22765-9a43-47a3-9c18-2f28667e10f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sample Size = 11314\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-55f10189ad3c>:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  news_df[\"clean_doc\"] = news_df[\"document\"].str.replace(\"[^a-zA-Z]\", \" \")                                                    # 특수문자 제거\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            document  \\\n",
            "0  Well i'm not sure about the story nad it did s...   \n",
            "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...   \n",
            "2  Although I realize that principle is not one o...   \n",
            "3  Notwithstanding all the legitimate fuss about ...   \n",
            "4  Well, I will have to change the scoring on my ...   \n",
            "\n",
            "                                           clean_doc  \n",
            "0  well sure about story seem biased what disagre...  \n",
            "1  yeah expect people read actually accept hard a...  \n",
            "2  although realize that principle your strongest...  \n",
            "3  notwithstanding legitimate fuss about this pro...  \n",
            "4  well will have change scoring playoff pool unf...  \n",
            "False\n",
            "\n",
            "True\n",
            "Total Sample Size = 10995\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:1970: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  result = asarray(a).shape\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/function_base.py:4454: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = asarray(arr)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10995,)\n",
            "\n",
            "Total Sample Size = 10940\n",
            "\n",
            "[[9, 59, 603, 207, 3278, 1495, 474, 702, 9470, 13686, 5533, 15227, 702, 442, 702, 70, 1148, 1095, 1036, 20294, 984, 705, 4294, 702, 217, 207, 1979, 15228, 13686, 4865, 4520, 87, 1530, 6, 52, 149, 581, 661, 4406, 4988, 4866, 1920, 755, 10668, 1102, 7837, 442, 957, 10669, 634, 51, 228, 2669, 4989, 178, 66, 222, 4521, 6066, 68, 4295], [1026, 532, 2, 60, 98, 582, 107, 800, 23, 79, 4522, 333, 7838, 864, 421, 3825, 458, 6488, 458, 2700, 4730, 333, 23, 9, 4731, 7262, 186, 310, 146, 170, 642, 1260, 107, 33568, 13, 985, 33569, 33570, 9471, 11491]]\n",
            "\n",
            "Word Vocabulary Size = 64277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]     # Negative Sampling\n",
        "print(\"Execution Time =\", time.time() - start_time)\n",
        "print(np.array(skip_grams).shape)\n",
        "print()\n",
        "\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]          # 첫번째 sample인 skip_grams[0] 내의 skipgrams로 형성된 dataset 확인\n",
        "print(\"Shape of Pairs, Labels =\", np.array(pairs).shape, np.array(labels).shape)\n",
        "print()\n",
        "\n",
        "for i in range(10):\n",
        "    print(\"{:s}({:d})  |  {:s}({:d})  ==>  {:d}\".format(idx2word[pairs[i][0]], pairs[i][0], idx2word[pairs[i][1]], pairs[i][1], labels[i]))\n",
        "print()\n",
        "\n",
        "pairs, labels = skip_grams[1][0], skip_grams[1][1]          # 첫번째 sample인 skip_grams[0] 내의 skipgrams로 형성된 dataset 확인\n",
        "print(\"Shape of Pairs, Labels =\", np.array(pairs).shape, np.array(labels).shape)\n",
        "print()\n",
        "\n",
        "for i in range(10):\n",
        "    print(\"{:s}({:d})  |  {:s}({:d})  ==>  {:d}\".format(idx2word[pairs[i][0]], pairs[i][0], idx2word[pairs[i][1]], pairs[i][1], labels[i]))\n",
        "print()"
      ],
      "metadata": {
        "id": "7HdodCrVSOLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "651ef510-ecd1-4d4d-9034-c59b1976b641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time = 0.06290721893310547\n",
            "(10, 2)\n",
            "\n",
            "Shape of Pairs, Labels = (2220, 2) (2220,)\n",
            "\n",
            "media(702)  |  occured(4294)  ==>  1\n",
            "received(634)  |  monsarrat(19945)  ==>  0\n",
            "clearly(661)  |  atrocities(4406)  ==>  1\n",
            "lived(1148)  |  unwrapped(41978)  ==>  0\n",
            "acts(1102)  |  articles(829)  ==>  0\n",
            "daily(1920)  |  think(6)  ==>  1\n",
            "europeans(4520)  |  whole(217)  ==>  1\n",
            "makes(228)  |  acts(1102)  ==>  1\n",
            "seem(207)  |  skys(34615)  ==>  0\n",
            "shame(4988)  |  learned(1845)  ==>  0\n",
            "\n",
            "Shape of Pairs, Labels = (1380, 2) (1380,)\n",
            "\n",
            "atheism(800)  |  streamflow(60409)  ==>  0\n",
            "chewables(33570)  |  forget(985)  ==>  1\n",
            "atheism(800)  |  faith(333)  ==>  1\n",
            "feelings(2700)  |  maybe(146)  ==>  1\n",
            "hard(107)  |  runs(421)  ==>  1\n",
            "much(13)  |  happily(7262)  ==>  1\n",
            "happily(7262)  |  sorry(458)  ==>  1\n",
            "ever(186)  |  well(9)  ==>  1\n",
            "ever(186)  |  willie(6920)  ==>  0\n",
            "feelings(2700)  |  gaetz(28101)  ==>  0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-d6987838b1d5>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  print(np.array(skip_grams).shape)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Window 크기 내에서, 각각의 뉴스 그룹 sample에 대해 동일한 프로세스 수행  \n",
        "- 중심 단어 - 주변 단어의 관계를 갖는 경우 **label=1**\n",
        "- 그렇지 않은 경우 **label=0**"
      ],
      "metadata": {
        "id": "fIcM6Iq8UDCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total Sample Size =\", len(skip_grams))\n",
        "\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "print(\"첫번째 paris =\", len(pairs))\n",
        "print(\"첫번째 paris =\", len(pairs))\n",
        "print()\n",
        "\n",
        "pairs, labels = skip_grams[1][0], skip_grams[1][1]\n",
        "print(\"두번째 paris =\", len(pairs))\n",
        "print(\"두번째 paris =\", len(pairs))\n",
        "print()\n",
        "\n",
        "start_time = time.time()\n",
        "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]      # 모든 뉴스 그룹에 대해 Negative Sampling 수행\n",
        "print(\"Execution Time =\", time.time() - start_time)\n",
        "print(\"Total Sample Size =\", len(skip_grams))"
      ],
      "metadata": {
        "id": "Jtr7qMwSTtj-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cde7092-bda4-4455-bed2-99ecb1fb1ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sample Size = 10\n",
            "첫번째 paris = 2220\n",
            "첫번째 paris = 2220\n",
            "\n",
            "두번째 paris = 1380\n",
            "두번째 paris = 1380\n",
            "\n",
            "Execution Time = 161.46300625801086\n",
            "Total Sample Size = 10940\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "## 중심 단어를 위한 Embedding Table\n",
        "w_inputs = Input(shape=(1, ), dtype=\"int32\")                        # Input이 one hot 인코딩이 아니라 정수 인코딩, input_length=1 => shape=(1,)\n",
        "word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)     # Embedding()은 (number of samples, input_length)인 2D 정수 텐서를 입력받아 \n",
        "                                                                    # (number of samples, input_length, embedding word dimentionality)인 3D float tensor 리턴\n",
        "\n",
        "## 주변 단어를 위한 Embedding Table\n",
        "c_inputs = Input(shape=(1, ), dtype=\"int32\")\n",
        "context_embedding = Embedding(vocab_size, embedding_dim)(c_inputs)\n",
        "print(word_embedding.shape, context_embedding.shape)\n",
        "print()\n",
        "\n",
        "dot_prod = Dot(axes=2)([word_embedding, context_embedding])\n",
        "dot_prod = Reshape((1, ), input_shape=(1, 1))(dot_prod)\n",
        "output = Activation(\"sigmoid\")(dot_prod)\n",
        "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "plot_model(model, show_shapes=True, show_layer_names=True)\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(1, 6):\n",
        "    loss = 0\n",
        "\n",
        "    for _, elem in enumerate(skip_grams):\n",
        "        first_elem = np.array(list(zip(*elem[0]))[0], dtype=\"int32\")            # skip_grams에서 elem[0] = pairs, elem[1] = label\n",
        "        second_elem = np.array(list(zip(*elem[0]))[1], dtype=\"int32\")\n",
        "        labels = np.array(elem[1], dtype=\"int32\")\n",
        "\n",
        "        X = [first_elem, second_elem]\n",
        "        Y = labels\n",
        "        loss += model.train_on_batch(X, Y)\n",
        "    \n",
        "    print(f\"Epoch = {epoch}/5  |  Loss = {loss}  |  Time = {time.time() - start_time}\")"
      ],
      "metadata": {
        "id": "s0gu4qGOVe57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264b5a6b-7a1c-4731-9749-4e5efccbc6bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100) (None, 1, 100)\n",
            "\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 1, 100)       6427700     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 1, 100)       6427700     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dot (Dot)                      (None, 1, 1)         0           ['embedding[0][0]',              \n",
            "                                                                  'embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 1)            0           ['dot[0][0]']                    \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 1)            0           ['reshape[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 12,855,400\n",
            "Trainable params: 12,855,400\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch = 1/5  |  Loss = 4631.936781760305  |  Time = 1754.3883123397827\n",
            "Epoch = 2/5  |  Loss = 3672.0121375508606  |  Time = 3505.384917974472\n",
            "Epoch = 3/5  |  Loss = 3508.9090659432113  |  Time = 5103.586159229279\n",
            "Epoch = 4/5  |  Loss = 3309.4603465329856  |  Time = 6687.711854219437\n",
            "Epoch = 5/5  |  Loss = 3087.125477953814  |  Time = 8257.850655078888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 실행 시간이 긴 관계로 학습된 모델을 별도로 저장\n",
        "model.save(\"./model/SGNS.h5\")\n",
        "\n",
        "if os.path.exists(\"./model/SGNS.h5\"):\n",
        "    model = load_model(\"./model/SGNS.h5\")\n",
        "else:\n",
        "    print(\"trained SGNS Model is not found\")\n",
        "\n",
        "\n",
        "## 학습된 embedding vector들을 vector.txt에 저장 후 genism 패키지를 통해 로드하여 손쉽게 단어 벡터 간 유사도 구하기\n",
        "file = open(\"./model/vector.txt\", \"w\")\n",
        "\n",
        "file.write(\"{} {}\\n\".format(vocab_size - 1, embedding_dim))\n",
        "vectors = model.get_weights()[0]\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    file.write(\"{} {}\\n\".format(word, \" \".join(map(str, list(vectors[i, :])))))\n",
        "\n",
        "file.close()\n",
        "\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format(\"./model/vector.txt\", binary=False)\n",
        "print(w2v.most_similar(positive=[\"soldiers\"]))\n",
        "print()\n",
        "print(w2v.most_similar(positive=[\"doctor\"]))\n",
        "print()\n",
        "print(w2v.most_similar(positive=[\"police\"]))\n",
        "print()\n",
        "print(w2v.most_similar(positive=[\"knife\"]))\n",
        "print()\n",
        "print(w2v.most_similar(positive=[\"engine\"]))"
      ],
      "metadata": {
        "id": "NyTZwSqbj8EQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83140732-8def-4ad5-d810-3b4f8987b13d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('troops', 0.8278371691703796), ('aviv', 0.8211274147033691), ('slaughtered', 0.8209096193313599), ('bombing', 0.8201167583465576), ('shelling', 0.8199048042297363), ('wounded', 0.8185893893241882), ('massacre', 0.8118721842765808), ('villages', 0.8063041567802429), ('massacred', 0.7978286743164062), ('azerbaijani', 0.7977402210235596)]\n",
            "\n",
            "[('disease', 0.654930830001831), ('lyme', 0.6340230107307434), ('pain', 0.624247670173645), ('infection', 0.5998868942260742), ('quack', 0.5956388711929321), ('clinic', 0.5894120931625366), ('symptoms', 0.5761964917182922), ('physician', 0.5554652214050293), ('yeast', 0.5554440021514893), ('systemic', 0.5554143190383911)]\n",
            "\n",
            "[('officers', 0.6939032077789307), ('unconstitutional', 0.6538252830505371), ('congress', 0.6325280070304871), ('enforcement', 0.6264446973800659), ('officials', 0.623386800289154), ('ruled', 0.6106305122375488), ('homicides', 0.5940735340118408), ('psychological', 0.5920382142066956), ('nuclear', 0.591476321220398), ('possession', 0.5860835313796997)]\n",
            "\n",
            "[('scant', 0.6582464575767517), ('races', 0.6499593257904053), ('charges', 0.6447961926460266), ('zionism', 0.6429561972618103), ('credible', 0.6402273178100586), ('salman', 0.6376645565032959), ('presenting', 0.6309700608253479), ('established', 0.6291972398757935), ('persecution', 0.6274126768112183), ('revolt', 0.6198505163192749)]\n",
            "\n",
            "[('nissan', 0.5806030035018921), ('tires', 0.5633103847503662), ('seat', 0.5364314317703247), ('ground', 0.5226004123687744), ('replaced', 0.5165414810180664), ('shifting', 0.4964064359664917), ('cylinder', 0.49566352367401123), ('cars', 0.4929234981536865), ('pipe', 0.4876146912574768), ('tank', 0.4868435859680176)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Topic Modeling(토픽 모델링) and LSA(Latent Semantic Analysis, 잠재 의미 분석)**"
      ],
      "metadata": {
        "id": "7BXbLIx2mVfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### **SVD(Singular Value Decomposition, 특이값 분해)**  \n",
        "- full SVD란 m x n matrix A에 대하여 다음과 같이 3개의 행렬의 곱으로 분해하는 방법  \n",
        "$$A=UΣV^\\text{T}$$  \n",
        "- truncated SVD란 full SVD로부터 얻은 3개의 행렬에서 일부 vector를 삭제하여 차원을 축소하는 방식  \n",
        "(https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)  \n",
        "  \n",
        "#### **LSA(Latent Semantic Analysis, 잠재 의미 분석)**  \n",
        "- 기존의 DTM과 TF-IDF의 단점을 보완하여 truncated SVD를 사용해 단어의 잠재적인 의미를 추가적으로 부여한 개선된 방식의 DTM"
      ],
      "metadata": {
        "id": "7FlZDeuqmrWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "  '먹고 싶은 사과',\n",
        "  '먹고 싶은 바나나',\n",
        "  '길고 노란 바나나 바나나',\n",
        "  '저는 과일이 좋아요'\n",
        "] \n",
        "vocab = list(set(word for doc in docs for word in doc.split()))\n",
        "vocab.sort()\n",
        "N = len(docs)\n",
        "\n",
        "def tf(term, doc):\n",
        "    return doc.count(term)\n",
        "\n",
        "result = []\n",
        "for a in range(N):\n",
        "    result.append([])\n",
        "    doc = docs[a]\n",
        "    for b in range(len(vocab)):\n",
        "        term = vocab[b]\n",
        "        result[-1].append(tf(term, doc))\n",
        "\n",
        "tf_pandas = pd.DataFrame(result, columns=vocab)\n",
        "tf_pandas"
      ],
      "metadata": {
        "id": "J7R5kKc7nygc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "8f45e16a-18b3-4ebc-c5c8-c2ace81cd1c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n",
              "0    0   0   0   1    0   1   1   0    0\n",
              "1    0   0   0   1    1   0   1   0    0\n",
              "2    0   1   1   0    2   0   0   0    0\n",
              "3    1   0   0   0    0   0   0   1    1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1a294e1-0583-49ba-a145-5de7a71ca741\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>과일이</th>\n",
              "      <th>길고</th>\n",
              "      <th>노란</th>\n",
              "      <th>먹고</th>\n",
              "      <th>바나나</th>\n",
              "      <th>사과</th>\n",
              "      <th>싶은</th>\n",
              "      <th>저는</th>\n",
              "      <th>좋아요</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1a294e1-0583-49ba-a145-5de7a71ca741')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e1a294e1-0583-49ba-a145-5de7a71ca741 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e1a294e1-0583-49ba-a145-5de7a71ca741');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = news_dataset.data\n",
        "print('# of samples=',len(documents))\n",
        "print(news_dataset.target_names)\n",
        "print()\n",
        "\n",
        "news_df = pd.DataFrame({'document' : documents})\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")                                        # 특수 문자 제거\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))       # 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())                                          # 전체 단어에 대한 소문자 변환\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])                   # 불용어 제거\n",
        "\n",
        "detokenized_doc = []                                                                                            # TF-IDF는 토큰화되어있지 않은 text를 사용하므로 역토큰화 진행\n",
        "for i in range(len(news_df)):\n",
        "    term = \" \".join(tokenized_doc[i])\n",
        "    detokenized_doc.append(term)\n",
        "news_df['clean_doc'] = detokenized_doc\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1000, max_df=0.5, smooth_idf=True)              # 상위 1000개의 단어로만 TF-IDF 행렬 생성\n",
        "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
        "print(\"TF-IDF matrix size =\", X.shape)\n",
        "print()\n",
        "\n",
        "svd_model = TruncatedSVD(n_components=20, algorithm=\"randomized\", n_iter=100, random_state=122)                 # Topic Modeling by Truncated SVD\n",
        "svd_model.fit(X)\n",
        "print(np.shape(svd_model.components_))\n",
        "print()\n",
        "\n",
        "def get_topics(components, feature_names, n=5):                                                                 # 20개의 row와 1000개의 column 에서 값이 가장 큰 5개를 찾아 단어로 출력\n",
        "    for idx, topic in enumerate(components):\n",
        "        print(\"Topic %d : \" % (idx + 1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
        "\n",
        "terms = vectorizer.get_feature_names()\n",
        "get_topics(svd_model.components_, terms)"
      ],
      "metadata": {
        "id": "7XHVrl8fvTMM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6899e9f-6634-4217-b1bf-4eed9b734521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of samples= 11314\n",
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-ffd97137531b>:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")                                        # 특수 문자 제거\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF matrix size = (11314, 1000)\n",
            "\n",
            "(20, 1000)\n",
            "\n",
            "Topic 1 :  [('like', 0.21386), ('know', 0.20046), ('people', 0.19293), ('think', 0.17805), ('good', 0.15128)]\n",
            "Topic 2 :  [('thanks', 0.32888), ('windows', 0.29088), ('card', 0.18069), ('drive', 0.17455), ('mail', 0.15111)]\n",
            "Topic 3 :  [('game', 0.37064), ('team', 0.32443), ('year', 0.28154), ('games', 0.2537), ('season', 0.18419)]\n",
            "Topic 4 :  [('drive', 0.53324), ('scsi', 0.20165), ('hard', 0.15628), ('disk', 0.15578), ('card', 0.13994)]\n",
            "Topic 5 :  [('windows', 0.40399), ('file', 0.25436), ('window', 0.18044), ('files', 0.16078), ('program', 0.13894)]\n",
            "Topic 6 :  [('chip', 0.16114), ('government', 0.16009), ('mail', 0.15625), ('space', 0.1507), ('information', 0.13562)]\n",
            "Topic 7 :  [('like', 0.67086), ('bike', 0.14236), ('chip', 0.11169), ('know', 0.11139), ('sounds', 0.10371)]\n",
            "Topic 8 :  [('card', 0.46633), ('video', 0.22137), ('sale', 0.21266), ('monitor', 0.15463), ('offer', 0.14643)]\n",
            "Topic 9 :  [('know', 0.46047), ('card', 0.33605), ('chip', 0.17558), ('government', 0.1522), ('video', 0.14356)]\n",
            "Topic 10 :  [('good', 0.42756), ('know', 0.23039), ('time', 0.1882), ('bike', 0.11406), ('jesus', 0.09027)]\n",
            "Topic 11 :  [('think', 0.78469), ('chip', 0.10899), ('good', 0.10635), ('thanks', 0.09123), ('clipper', 0.07946)]\n",
            "Topic 12 :  [('thanks', 0.36824), ('good', 0.22729), ('right', 0.21559), ('bike', 0.21037), ('problem', 0.20894)]\n",
            "Topic 13 :  [('good', 0.36212), ('people', 0.33985), ('windows', 0.28385), ('know', 0.26232), ('file', 0.18422)]\n",
            "Topic 14 :  [('space', 0.39946), ('think', 0.23258), ('know', 0.18074), ('nasa', 0.15174), ('problem', 0.12957)]\n",
            "Topic 15 :  [('space', 0.31613), ('good', 0.3094), ('card', 0.22603), ('people', 0.17476), ('time', 0.14496)]\n",
            "Topic 16 :  [('people', 0.48156), ('problem', 0.19961), ('window', 0.15281), ('time', 0.14664), ('game', 0.12871)]\n",
            "Topic 17 :  [('time', 0.34465), ('bike', 0.27303), ('right', 0.25557), ('windows', 0.1997), ('file', 0.19118)]\n",
            "Topic 18 :  [('time', 0.5973), ('problem', 0.15504), ('file', 0.14956), ('think', 0.12847), ('israel', 0.10903)]\n",
            "Topic 19 :  [('file', 0.44163), ('need', 0.26633), ('card', 0.18388), ('files', 0.17453), ('right', 0.15448)]\n",
            "Topic 20 :  [('problem', 0.33006), ('file', 0.27651), ('thanks', 0.23578), ('used', 0.19206), ('space', 0.13185)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **GloVe, 글로브**"
      ],
      "metadata": {
        "id": "fOR6MP6nydg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")\n",
        "\n",
        "targetXML = open(\"ted_en-20160408.xml\", \"r\", encoding=\"UTF8\")\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = \"\\n\".join(target_text.xpath(\"//content/text()\"))                   # XML 파일로부터 <content></content> tag 사이의 내용들만 가져옴\n",
        "content_text = re.sub(r\"\\([^)]*\\)\", \"\", parse_text)                             # re.sub()를 통해 배경음 부분 (Audio), (Laugther)등 제거\n",
        "sent_text = sent_tokenize(content_text)                                         # 입력 corpus에 대해 문장 토큰화\n",
        "\n",
        "normalized_text = []                                                            # 각 문장에 대해 구두점 제거 및 소문자화\n",
        "for string in sent_text:\n",
        "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "    normalized_text.append(tokens)\n",
        "\n",
        "result = [word_tokenize(sent) for sent in normalized_text]                      # 각 문장에 대해 단어 토큰화\n",
        "print(\"Total Sample size =\", len(result))\n",
        "print()\n",
        "\n",
        "corpus = Corpus()\n",
        "corpus.fit(result, window=5)                                                    # 학습에 사용할 동시 등장 행렬 생성, 윈도우 크기=5\n",
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)                 # corpus.matrix = 동시 등장 행렬\n",
        "glove.add_dictionary(corpus.dictionary)                                         # corpus 내부에는 dictionary로 저장된 각 단어의 index 정보에 대해 검색을 위해 GloVe 모델에 추가\n",
        "\n",
        "print(glove.most_similar(\"man\"))\n",
        "print(glove.most_similar(\"boy\"))\n",
        "print(glove.most_similar(\"university\"))\n",
        "print(glove.most_similar(\"water\"))\n",
        "print(glove.most_similar(\"physics\"))\n",
        "print(glove.most_similar(\"muscle\"))\n",
        "print(glove.most_similar(\"clean\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "GOPe2hRs85r3",
        "outputId": "49192c76-3240-4849-88ad-fdf66ad3790b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-84be397b0d08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mparse_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"//content/text()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                   \u001b[0;31m# XML 파일로부터 <content></content> tag 사이의 내용들만 가져옴\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcontent_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\([^)]*\\)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_text\u001b[0m\u001b[0;34m)\u001b[0m                             \u001b[0;31m# re.sub()를 통해 배경음 부분 (Audio), (Laugther)등 제거\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msent_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_text\u001b[0m\u001b[0;34m)\u001b[0m                                         \u001b[0;31m# 입력 corpus에 대해 문장 토큰화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnormalized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m                                                            \u001b[0;31m# 각 문장에 대해 구두점 제거 및 소문자화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \"\"\"\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \"\"\"\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \"\"\"\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \"\"\"\n\u001b[1;32m   1420\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m             \u001b[0msentence1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1378\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m             \u001b[0;31m# Find the word before the current match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m             \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxsplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m             \u001b[0mbefore_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0mbefore_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FastText, 패스트 텍스트**"
      ],
      "metadata": {
        "id": "M5ahiNLmAFE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "targetXML = open(\"ted_en-20160408.xml\", \"r\", encoding=\"UTF8\")\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = \"\\n\".join(target_text.xpath(\"//content/text()\"))                   # XML 파일로부터 <content></content> tag 사이의 내용들만 가져옴\n",
        "content_text = re.sub(r\"\\([^)]*\\)\", \"\", parse_text)                             # re.sub()를 통해 배경음 부분 (Audio), (Laugther)등 제거\n",
        "sent_text = sent_tokenize(content_text)                                         # 입력 corpus에 대해 문장 토큰화\n",
        "\n",
        "normalized_text = []                                                            # 각 문장에 대해 구두점 제거 및 소문자화\n",
        "for string in sent_text:\n",
        "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "    normalized_text.append(tokens)\n",
        "\n",
        "result = [word_tokenize(sent) for sent in normalized_text]                      # 각 문장에 대해 단어 토큰화\n",
        "print(\"Total Sample size =\", len(result))\n",
        "print(\"Execution Time =\", time.time()-start_time)\n",
        "print()"
      ],
      "metadata": {
        "id": "PyCtT7_0ALqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Word2Vec과 FastText의 비교**"
      ],
      "metadata": {
        "id": "IAJDy-epAwsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)\n",
        "w2v_model.wv.most_similar(\"electrofishing\")"
      ],
      "metadata": {
        "id": "9D4FB6JBA0YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext_model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1)\n",
        "fasttext_model.wv.most_similar(\"electrofishing\")"
      ],
      "metadata": {
        "id": "2sVaQoPgBJi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-trained Word Embedding, 사전 학습된 워드 임베딩**"
      ],
      "metadata": {
        "id": "M8Y6e-htBicO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Keras Embedding Layer, 케라스 임베딩 층**"
      ],
      "metadata": {
        "id": "5UAdeCKPzsZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"nice great best amazing\", \"stop lies\", \"pitiful nerd\", \"excellent work\", \"supreme quality\", \"bad\", \"highly respectable\"]\n",
        "y_train = [1, 0, 0, 1, 1, 0, 1]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"Vocabulary Size = {vocab_size}\")\n",
        "print()\n",
        "\n",
        "X_encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print(f\"Encoded X = {X_encoded}\")\n",
        "print()\n",
        "\n",
        "max_len = max(len(word) for word in X_encoded)\n",
        "print(f\"Maximum Length = {max_len}\")\n",
        "print()\n",
        "\n",
        "X_train = pad_sequences(X_encoded, maxlen=max_len, padding=\"post\")\n",
        "y_train = np.array(y_train)\n",
        "print(X_train)\n",
        "print()"
      ],
      "metadata": {
        "id": "fxCLHSpgzrBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 4\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.summary()\n",
        "print()\n",
        "plot_model(model, show_shapes=True)\n",
        "print()\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metric=[\"acc\"])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "metadata": {
        "id": "Zv2KSL9M1BJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pre-trained GloVe, 사전 학습된 GloVe의 사용**"
      ],
      "metadata": {
        "id": "rO_H2S1PzzKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
        "zip_file = zipfile.ZipFile('glove.6B.zip')\n",
        "zip_file.extractall() \n",
        "zip_file.close()\n",
        "\n",
        "embedding_dict = dict()\n",
        "\n",
        "file = open(\"glove.6B.100d.txt\", encoding=\"utf8\")\n",
        "for line in file:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]                                                       # word_vector[0] = word string, word_vector[1:] = embedding vector\n",
        "\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype=\"float32\")              # 100개의 값을 갖는 array로 변환\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "file.close()\n",
        "print(f\"# of Embedding Vectors = {len(embedding_dict)}\")\n",
        "print()\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "print(np.shape(embedding_matrix))\n",
        "print(tokenizer.word_index.items())\n",
        "print()\n",
        "\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    vector_value = embedding_dict.get(word)                                     # 단어와 mapping되는 사전 학습된 embedding vector\n",
        "    if vector_value is not None:\n",
        "        embedding_matrix[index] = vector_value\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=True))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.summary()\n",
        "plot_model(model, show_shapes=True)\n",
        "print()\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "metadata": {
        "id": "Q-VUEqurz0w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pre-trained Word2Vec, 사전 학습된 Word2Vec의 사용**"
      ],
      "metadata": {
        "id": "sIQs-xjRzzfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", filename=\"GoogleNews-vectors-negative300.bin.gz\")\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "print(word2vec_model.vectors.shape)         # 모델의 size 확인\n",
        "print()\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "print(np.shape(embedding_matrix))\n",
        "print()\n",
        "\n",
        "def get_vector(word):\n",
        "    if word in word2vec_model:\n",
        "        return word2vec_model[word]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    vector_value = get_vector(word)\n",
        "    if vector_value is not None:\n",
        "        embedding_matrix[index] = vector_value\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(max_len, ), dtype=\"int32\"))\n",
        "model.add(Embedding(vocab_size, 300, weights=[embeddig_matrix], input_length=max_len, trainable=True))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "model.fit(X_train, y_train, epochs=100, verbose=2)"
      ],
      "metadata": {
        "id": "aVHQtphTz00Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}